{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ① 市町村別気象データを都道府県気象データに加工する"
      ],
      "metadata": {
        "id": "yCDtWuDCyst2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モジュールのインポート"
      ],
      "metadata": {
        "id": "7O5QbzPjyipt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "qaGsLHjl4EXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "関数"
      ],
      "metadata": {
        "id": "OxG_Vexr4YMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nan_check():\n",
        "  for t_df in df_list:\n",
        "    print(t_df.head())\n",
        "    # 各列のNaNの確認\n",
        "    print(t_df['temp'].hasnans)\n",
        "    print(t_df['prec'].hasnans)\n",
        "    print(t_df['sun'].hasnans)\n",
        "    print(t_df['snow'].hasnans)\n",
        "    print(t_df['humi'].hasnans)"
      ],
      "metadata": {
        "id": "YwlJC-K94XeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ファイルの取り込み\n",
        "1. Numbersで処理したcsvを都道府県ごとにcolab上にアップロード\n",
        "\n",
        "2. 市町村全てのデータを取り込む\n",
        "\n",
        "愛知県のデータ例: aa_22_m.csv, ag_22_m.csv, ai_22_m.csv, am_22_m.csv, ao_22_m.csv, as_22_m.csv, at_22_m.csv, aty_22_m.csv"
      ],
      "metadata": {
        "id": "kz1Ui8yNwcqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = glob.glob('/content/*.csv')\n",
        "print(type(file_list))\n",
        "print(file_list)"
      ],
      "metadata": {
        "id": "w67fD5xetvdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ファイルの欠損確認\n",
        "1. pandasをインポート\n",
        "\n",
        "2. 市町村データファイルをデータフレームに変換する\n",
        "\n",
        "3. データを確認する\n",
        "\n",
        "4. データのinfoを確認する\n",
        "\n",
        "5. 各気象データの品質の数を確認する\n",
        "\n",
        "6. 各気象データのNaNを確認する"
      ],
      "metadata": {
        "id": "hnL-nbduVXzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "for f in file_list:\n",
        "  df= pd.read_csv(f) # 個別にファイル名を指定\n",
        "  print(df.head())\n",
        "  df.info()\n",
        "  print(df['temp_q'].value_counts())\n",
        "  print(df['prec_q'].value_counts())\n",
        "  print(df['sun_q'].value_counts())\n",
        "  print(df['snow_q'].value_counts())\n",
        "  print(df['humi_q'].value_counts())\n",
        "\n",
        "  # 各列のNaNの確認\n",
        "  print(df['temp'].hasnans)\n",
        "  print(df['prec'].hasnans)\n",
        "  print(df['sun'].hasnans)\n",
        "  print(df['snow'].hasnans)\n",
        "  print(df['humi'].hasnans)\n"
      ],
      "metadata": {
        "id": "B383k-tuVMY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "（オプション）品質情報1のカラムのデータ確認\n",
        "\n",
        "※ 品質情報1:欠損、品質情報4:データとしての扱いに注意、品質情報5:データとして使用可能、品質情報8:正常データ"
      ],
      "metadata": {
        "id": "PQQnhlRDVwSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.query('temp_q == 1')"
      ],
      "metadata": {
        "id": "vn14rHPuVp7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#欠損データを埋める\n",
        "\n",
        "欠損データの処理\n",
        "気温データの場合例：\n",
        "\n",
        "データdf['temp']がNaNかつ品質番号df['temp_q']が1である時、前後データから予測して欠損値を埋める。\n",
        "\n",
        "それ以外の場合のNaNは、0で埋める\n",
        "\n",
        "欠損値処理したデータからデータフレームを作成\n",
        "\n",
        "データフレームをリスト化する"
      ],
      "metadata": {
        "id": "skMXf1zrWdla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_list = []\n",
        "for f in file_list:\n",
        "  df= pd.read_csv(f) # 個別にファイル名を指定\n",
        "  if df['temp'].hasnans and 1 in df['temp_q'].values:\n",
        "    df['temp'].interpolate(inplace= True, limit_direction='both')\n",
        "  else:\n",
        "    df['temp'].fillna(0,inplace = True)\n",
        "\n",
        "  if df['prec'].hasnans and 1 in df['prec_q'].values:\n",
        "    df['prec'].interpolate(inplace= True, limit_direction='both')\n",
        "  else:\n",
        "    df['prec'].fillna(0,inplace = True)\n",
        "\n",
        "  if df['sun'].hasnans and 1 in df['sun_q'].values:\n",
        "    df['sun'].interpolate(inplace= True, limit_direction='both')\n",
        "  else:\n",
        "    df['sun'].fillna(0,inplace = True)\n",
        "\n",
        "  if df['snow'].hasnans and 1 in df['snow_q'].values:\n",
        "    df['snow'].interpolate(inplace= True, limit_direction='both')\n",
        "  else:\n",
        "    df['snow'].fillna(0,inplace = True)\n",
        "\n",
        "  if df['humi'].hasnans and 1 in df['humi_q'].values:\n",
        "    df['humi'].interpolate(inplace= True, limit_direction='both')\n",
        "  else:\n",
        "    df['humi'].fillna(0,inplace = True)\n",
        "\n",
        " #削除ではなく抽出\n",
        "  t_df = pd.DataFrame(df, columns= ['temp', 'prec', 'sun', 'snow', 'humi'])\n",
        "\n",
        "  # df_listに追加する。\n",
        "  df_list.append(t_df)\n",
        "\n",
        "df_list"
      ],
      "metadata": {
        "id": "O9KE3y7pWLti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "（オプション）欠損値データ確認"
      ],
      "metadata": {
        "id": "yPIBTV3ZV92R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nan_check()"
      ],
      "metadata": {
        "id": "DRKRTnhzVn1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#数値を平均してデータフレームを結合する\n",
        "\n",
        "欠損処理した市町村データを気象条件ごとに平均値および標準偏差を算出し1つの都道府県データとする。"
      ],
      "metadata": {
        "id": "f7eqoNDfXQIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_m = pd.DataFrame(np.mean([df_list[0], df_list[1], df_list[2], df_list[3], df_list[4], df_list[5], df_list[6], df_list[7]], 0), columns=t_df.columns)\n",
        "df_s = pd.DataFrame(np.std([df_list[0], df_list[1], df_list[2], df_list[3], df_list[4], df_list[5], df_list[6], df_list[7]], 0), columns=t_df.columns)\n",
        "\n",
        "\n",
        "#平均の調節\n",
        "df_m['snow'] = df_m['snow'] *8/4\n",
        "df_m['humi'] = df_m['humi'] *8/1\n",
        "\n",
        "rename_std()\n",
        "\n",
        "all_df = pd.merge(df[['date']], df_m, left_index=True, right_index= True)\n",
        "\n",
        "pref_df = pd.merge(all_df, df_s[['temp_s', 'prec_s', 'sun_s', 'snow_s', 'humi_s']], left_index=True, right_index= True)\n",
        "\n",
        "pref_df.head()"
      ],
      "metadata": {
        "id": "YpF0WnWY34QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#市町村データから都道府県データにまとめたものをcsv形式で保存する"
      ],
      "metadata": {
        "id": "t8xUrOGgyGJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pref_df.to_csv('pref_df.csv')"
      ],
      "metadata": {
        "id": "ikdu4Wl05QLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ② 気象データにブロッコリー収穫量データを結合する"
      ],
      "metadata": {
        "id": "dZwv1czgzD8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モジュールのインポート"
      ],
      "metadata": {
        "id": "cwf-AqK_zgFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mport pandas as pd"
      ],
      "metadata": {
        "id": "YuneBZ3xycY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ブロッコリー収穫量をデータフレームに結合し、気象データごとに分割して保存する"
      ],
      "metadata": {
        "id": "_V6W0fOFIg6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 収穫量をデータフレームに追加して気象データごとにファイルを分割して保存する\n",
        "\n",
        "# csvファイルを取り込む\n",
        "df = pd.read_csv('/content/pref_df.csv')\n",
        "\n",
        "# ブロッコリー収穫量をカラムに追加する\n",
        "df['Broc']= 1410\n",
        "\n",
        "# 気温データのデータフレームを作成して、保存\n",
        "Br_temp = df[['date','temp','Broc']]\n",
        "Br_temp.to_csv('tBroc_pref_df.csv', index=False)\n",
        "\n",
        "# 降水量データのデータフレームを作成して、保存\n",
        "Br_prec = df[['date', 'prec', 'Broc']]\n",
        "Br_prec.to_csv('pBroc_pref_df.csv', index=False)\n",
        "\n",
        "# 日照時間データのデータフレームを作成して、保存\n",
        "Br_sun = df[['date', 'sun', 'Broc']]\n",
        "Br_sun.to_csv('sBroc_pref_df.csv', index=False)\n",
        "\n",
        "# 降雪量データのデータフレームを作成して、保存\n",
        "Br_snow = df[['date', 'snow', 'Broc']]\n",
        "Br_snow.to_csv('snBroc_pref_df.csv', index=False)\n",
        "\n",
        "# 湿度データのデータフレームを作成して、保存\n",
        "Br_humi = df[['date', 'humi', 'Broc']]\n",
        "Br_humi.to_csv('hBroc_pref_df.csv', index=False)\n",
        "\n",
        "# ブロッコリーデータをpref_dfに加えて、保存\n",
        "df.to_csv('pref_df.csv', index=False)"
      ],
      "metadata": {
        "id": "NAlXfmuk4qUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 線形回帰による収穫量の予測\n",
        "\n",
        "1. モジュールをインポートする。\n",
        "\n",
        "2. csvファイルからデータセットのリストを作る\n",
        "\n",
        "3. 学習データとテストデータを分割する\n",
        "\n",
        "4. モデルの評価"
      ],
      "metadata": {
        "id": "B5sY-8tl1QYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#ファイルを読み込む\n",
        "file_list = glob.glob('/content/*.csv')\n",
        "print(file_list) #取り込んだファイルの順序を表示する\n",
        "\n",
        "Broc = []\n",
        "data_list = []\n",
        "for file in file_list:\n",
        "  df = pd.read_csv(file)\n",
        "  Broc.append(df['Broc'][0]) # ブロッコリー収穫量をリスト化する　Xにあたる\n",
        "\n",
        "  df = df[['temp','prec', 'sun', 'snow', 'humi']] # 気象データ5条件を取り出す\n",
        "  data_list.append(df.values.ravel()) # (365, 5)データを一次元配列にして、読み込みファイル分リスト化する　yにあたる\n",
        "\n",
        "# 線形で回帰する\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_list, Broc, test_size= 0.3, random_state=0)\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test,y_test)\n",
        "model.predict(X_test)"
      ],
      "metadata": {
        "id": "fVZOwXXS2HQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ランダムフォレスト回帰による収穫量の予測\n",
        "\n",
        "1. モジュールをインポートする。\n",
        "\n",
        "2. csvファイルからデータセットのリストを作る\n",
        "\n",
        "3. 学習データとテストデータを分割する\n",
        "\n",
        "4. モデルの評価"
      ],
      "metadata": {
        "id": "KyCa_Uqk9EpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#ファイルを読み込む\n",
        "file_list = glob.glob('/content/*.csv')\n",
        "print(file_list) #取り込んだファイルの順序を表示する\n",
        "\n",
        "Broc = []\n",
        "data_list = []\n",
        "for file in file_list:\n",
        "  df = pd.read_csv(file)\n",
        "  Broc.append(df['Broc'][0]) # ブロッコリー収穫量をリスト化する　Xにあたる\n",
        "\n",
        "  df = df[['temp','prec', 'sun', 'snow', 'humi']] # 気象データ5条件を取り出す\n",
        "  data_list.append(df.values.ravel()) # (365, 5)データを一次元配列にして、読み込みファイル分リスト化する　yにあたる\n",
        "\n",
        "# ランダムフォレストで回帰する場合\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_list, Broc, test_size= 0.3, random_state=0)\n",
        "model = RandomForestRegressor(n_estimators=10, criterion='squared_error', max_depth=None)\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test,y_test)\n",
        "model.predict(X_test)"
      ],
      "metadata": {
        "id": "Zu3_8nB58JjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 収穫量をクラスタリングし、予測データに用いる\n",
        "都道府県ファイルを読み込む\n",
        "\n",
        "各気象条件ごとにデータをリスト化する\n"
      ],
      "metadata": {
        "id": "-mlHIsHupvwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#全てのファイルを読み込む\n",
        "file_list = glob.glob('/content/*.csv')\n",
        "\n",
        "# 正規表現を使って、気象データごとにファイルをリスト化する\n",
        "import re\n",
        "\n",
        "# 気温データ19道府県をリスト化する\n",
        "file_name_pattern = re.compile(r'.*tBroc.*')\n",
        "file_t =[]\n",
        "for file_name in file_list:\n",
        "  # ファイル名に対する正規表現パターンが一致するか確認\n",
        "  if file_name_pattern.match(file_name):\n",
        "    file_t.append(file_name)\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "# 日照時間データ19道府県をリスト化する\n",
        "file_name_pattern = re.compile(r'.*sBroc.*')\n",
        "file_s =[]\n",
        "for file_name in file_list:\n",
        "  # ファイル名に対する正規表現パターンが一致するか確認\n",
        "  if file_name_pattern.match(file_name):\n",
        "    file_s.append(file_name)\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "# 降水量データ19道府県をリスト化する\n",
        "file_name_pattern = re.compile(r'.*pBroc.*')\n",
        "file_p =[]\n",
        "for file_name in file_list:\n",
        "  # ファイル名に対する正規表現パターンが一致するか確認\n",
        "  if file_name_pattern.match(file_name):\n",
        "    file_p.append(file_name)\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "# 降雪量データ19道府県をリスト化する\n",
        "file_name_pattern = re.compile(r'.*snBroc.*')\n",
        "file_sn =[]\n",
        "for file_name in file_list:\n",
        "  # ファイル名に対する正規表現パターンが一致するか確認\n",
        "  if file_name_pattern.match(file_name):\n",
        "    file_sn.append(file_name)\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "# 湿度データ19道府県をリスト化する\n",
        "file_name_pattern = re.compile(r'.*hBroc.*')\n",
        "file_h =[]\n",
        "for file_name in file_list:\n",
        "  # ファイル名に対する正規表現パターンが一致するか確認\n",
        "  if file_name_pattern.match(file_name):\n",
        "    file_h.append(file_name)\n",
        "  else:\n",
        "    continue"
      ],
      "metadata": {
        "id": "2fzPEKNxp5Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "各気象条件データをクラスタリングする\n"
      ],
      "metadata": {
        "id": "7s1s_o1WLbyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 湿度\n",
        "\n",
        "# リストにデータを追加する\n",
        "Broc = []\n",
        "Humi = []\n",
        "\n",
        "for f in file_h:\n",
        "  df= pd.read_csv(f) # 個別にファイル名を指定\n",
        "\n",
        "  df = df.T #行列を入れ替える\n",
        "\n",
        "  df.columns = df.iloc[0] #日付をカラムにする\n",
        "\n",
        "  df = df.drop(df.index[0]) #日付行を削除する\n",
        "\n",
        "  # リストにデータを追加する\n",
        "  Broc.append(df.loc['Broc'][0]) #BrocリストにBrocの値を追加\n",
        "\n",
        "  Humi.append(df.loc['humi'].values) # TempリストにTempの値を追加\n",
        "\n",
        "# KMeansでクラスタリング\n",
        "model = KMeans(n_clusters=4, random_state=0)\n",
        "model.fit(Humi)\n",
        "\n",
        "df_humi = pd.DataFrame(data = Humi)\n",
        "\n",
        "df_humi['クラス'] = model.labels_\n",
        "df_m['humi_c']=df_humi['クラス']\n",
        "import re\n",
        "\n",
        "# 正規表現パターン\n",
        "pattern = re.compile(r'_([A-Z]+)_')\n",
        "pref_list = []\n",
        "for f in file_h:\n",
        "  match = pattern.search(f) # ファイル名から正規表現に一致する部分を検索\n",
        "  if match: # 一致した部分を取り出す\n",
        "    result = match.group(1)\n",
        "    pref_list.append(result)\n",
        "  else:\n",
        "    print('Match not found.')\n",
        "\n",
        "df_humi['pref'] = pref_list\n",
        "df_humi['humi_c']=df_humi['クラス']\n",
        "\n",
        "# 日照時間\n",
        "\n",
        "# リストにデータを追加する\n",
        "Broc = []\n",
        "Sun = []\n",
        "\n",
        "for f in file_s:\n",
        "  df= pd.read_csv(f) # 個別にファイル名を指定\n",
        "\n",
        "  df = df.T #行列を入れ替える\n",
        "\n",
        "  df.columns = df.iloc[0] #日付をカラムにする\n",
        "\n",
        "  df = df.drop(df.index[0]) #日付行を削除する\n",
        "\n",
        "  # リストにデータを追加する\n",
        "  Broc.append(df.loc['Broc'][0]) #BrocリストにBrocの値を追加\n",
        "\n",
        "  Sun.append(df.loc['sun'].values) # TempリストにTempの値を追加\n",
        "\n",
        "\n",
        "# KMeansでクラスタリング\n",
        "model = KMeans(n_clusters=4, random_state=0)\n",
        "model.fit(Sun)\n",
        "\n",
        "df_sun = pd.DataFrame(data = Sun)\n",
        "\n",
        "df_sun['クラス'] = model.labels_\n",
        "df_m['sun_c']=df_sun['クラス']\n",
        "\n",
        "import re\n",
        "\n",
        "# 正規表現パターン\n",
        "pattern = re.compile(r'_([A-Z]+)_')\n",
        "pref_list = []\n",
        "for f in file_s:\n",
        "  match = pattern.search(f) # ファイル名から正規表現に一致する部分を検索\n",
        "  if match: # 一致した部分を取り出す\n",
        "    result = match.group(1)\n",
        "    pref_list.append(result)\n",
        "  else:\n",
        "    print('Match not found.')\n",
        "\n",
        "df_sun['pref'] = pref_list\n",
        "df_sun['sun_c']=df_prec['クラス']\n",
        "\n",
        "\n",
        "#　降水量\n",
        "\n",
        "# リストにデータを追加する\n",
        "Broc = []\n",
        "Prec = []\n",
        "\n",
        "for f in file_p:\n",
        "  df= pd.read_csv(f) # 個別にファイル名を指定\n",
        "\n",
        "  df = df.T #行列を入れ替える\n",
        "\n",
        "  df.columns = df.iloc[0] #日付をカラムにする\n",
        "\n",
        "  df = df.drop(df.index[0]) #日付行を削除する\n",
        "\n",
        "  # リストにデータを追加する\n",
        "  Broc.append(df.loc['Broc'][0]) #BrocリストにBrocの値を追加\n",
        "\n",
        "  Prec.append(df.loc['prec'].values) # PrecリストにPrecの値を追加\n",
        "\n",
        "#KMeansでクラスタリング　クラスタ4\n",
        "model = KMeans(n_clusters=4, random_state=0)\n",
        "model.fit(Prec)\n",
        "\n",
        "df_prec = pd.DataFrame(data = Prec)\n",
        "\n",
        "df_prec['クラス'] = model.labels_\n",
        "df_m['prec_c']=df_prec['クラス']\n",
        "\n",
        "import re\n",
        "\n",
        "# 正規表現パターン\n",
        "pattern = re.compile(r'_([A-Z]+)_')\n",
        "pref_list = []\n",
        "for f in file_p:\n",
        "  match = pattern.search(f) # ファイル名から正規表現に一致する部分を検索\n",
        "  if match: # 一致した部分を取り出す\n",
        "    result = match.group(1)\n",
        "    pref_list.append(result)\n",
        "  else:\n",
        "    print('Match not found.')\n",
        "\n",
        "df_prec['pref'] = pref_list\n",
        "df_prec['prec_c']=df_prec['クラス']\n",
        "\n",
        "\n",
        "#雪\n",
        "\n",
        "# リストにデータを追加する\n",
        "Broc = []\n",
        "Snow = []\n",
        "\n",
        "for f in file_sn:\n",
        "  df= pd.read_csv(f) # 個別にファイル名を指定\n",
        "\n",
        "  df = df.T #行列を入れ替える\n",
        "\n",
        "  df.columns = df.iloc[0] #日付をカラムにする\n",
        "\n",
        "  df = df.drop(df.index[0]) #日付行を削除する\n",
        "\n",
        "  # リストにデータを追加する\n",
        "  Broc.append(df.loc['Broc'][0]) #BrocリストにBrocの値を追加\n",
        "\n",
        "  Snow.append(df.loc['snow'].values) # TempリストにTempの値を追加\n",
        "\n",
        "\n",
        "#KMeansでクラスタリング クラスタ4\n",
        "model = KMeans(n_clusters=4, random_state=0)\n",
        "model.fit(Snow)\n",
        "\n",
        "df_snow = pd.DataFrame(data = Snow)\n",
        "\n",
        "df_snow['クラス'] = model.labels_\n",
        "df_m['snow_c']=df_snow['クラス']\n",
        "\n",
        "Date = df.columns.tolist()\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "# 正規表現パターン\n",
        "pattern = re.compile(r'_([A-Z]+)_')\n",
        "pref_list = []\n",
        "for f in file_sn:\n",
        "  match = pattern.search(f) # ファイル名から正規表現に一致する部分を検索\n",
        "  if match: # 一致した部分を取り出す\n",
        "    result = match.group(1)\n",
        "    pref_list.append(result)\n",
        "  else:\n",
        "    print('Match not found.')\n",
        "\n",
        "df_snow['pref'] = pref_list\n",
        "df_snow['snow_c']=df_snow['クラス']\n",
        "\n",
        "\n",
        "#　気温\n",
        "\n",
        "# リストにデータを追加する\n",
        "Broc = []\n",
        "Temp = []\n",
        "\n",
        "for f in file_t:\n",
        "  df= pd.read_csv(f) # 個別にファイル名を指定\n",
        "\n",
        "  df = df.T #行列を入れ替える\n",
        "\n",
        "  df.columns = df.iloc[0] #日付をカラムにする\n",
        "\n",
        "  df = df.drop(df.index[0]) #日付行を削除する\n",
        "\n",
        "  # リストにデータを追加する\n",
        "  Broc.append(df.loc['Broc'][0]) #BrocリストにBrocの値を追加\n",
        "\n",
        "  Temp.append(df.loc['temp'].values) # TempリストにTempの値を追加\n",
        "\n",
        "#KMeansでクラスタリング\n",
        "model = KMeans(n_clusters=4, random_state=0)\n",
        "model.fit(Temp)\n",
        "\n",
        "df_temp = pd.DataFrame(data = Temp)\n",
        "\n",
        "df_temp['クラス'] = model.labels_\n",
        "df_m['temp_c']=df_temp['クラス']\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "# 正規表現パターン\n",
        "pattern = re.compile(r'_([A-Z]+)_')\n",
        "pref_list = []\n",
        "for f in file_t:\n",
        "  match = pattern.search(f) # ファイル名から正規表現に一致する部分を検索\n",
        "  if match: # 一致した部分を取り出す\n",
        "    result = match.group(1)\n",
        "    pref_list.append(result)\n",
        "  else:\n",
        "    print('Match not found.')\n",
        "\n",
        "df_temp['pref'] = pref_list\n",
        "df_temp['temp_c']=df_temp['クラス']\n",
        "df_temp['Broc'] = Broc\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# カラムAとBを結合する\n",
        "result_df = pd.merge(df_temp[['temp_c', 'pref','Broc']],\n",
        "                     df_snow[['snow_c', 'pref']],\n",
        "                     on='pref')\n",
        "\n",
        "result_df = pd.merge(result_df,\n",
        "                     df_humi[['humi_c', 'pref']],\n",
        "                     on='pref')\n",
        "\n",
        "result_df = pd.merge(result_df,\n",
        "                     df_sun[['sun_c', 'pref']],\n",
        "                     on='pref')\n",
        "\n",
        "result_df = pd.merge(result_df,\n",
        "                     df_prec[['prec_c', 'pref']],\n",
        "                     on='pref')\n",
        "\n",
        "\n",
        "result_df\n"
      ],
      "metadata": {
        "id": "fnc2K0Z5ryut"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}